{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Latent Semantic Analysis\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Latent Semantic Analysis (LSA) is a framework for analyzing text using matrices\n",
      "* Find relationships between documents and terms within documents\n",
      "* Used for document classification, clustering, text search, and more\n",
      "* Lots of experts here at CU Boulder!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "sci-kit learn"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* sci-kit learn is a Python library for doing machine learning, feature selection, etc.\n",
      "* Integrates with numpy and scipy\n",
      "* Great documentation and tutorials"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Vectorizing text"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Most machine-learning and statistical algorithms only work with structured, tabular data\n",
      "* A simple way to add structure to text is to use a document-term matrix"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Getting Briefs from django"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#from IPython.lib import passwd\n",
      "#passwd()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os, sys\n",
      "os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"web.settings\")\n",
      "sys.path.append('/vagrant/')\n",
      "from brief.models import Brief  \n",
      "import nltk\n",
      "from django.core.management.base import BaseCommand\n",
      "from optparse import make_option           \n",
      "from datetime import datetime, timedelta   "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "example  = list()\n",
      "title = list()\n",
      "item = Brief.objects.values()\n",
      "for i in range(500):\n",
      "    if (item[i]['language'] == \"en\"):\n",
      "        try:\n",
      "            temp= str(item[i]['body_html'])\n",
      "            temp_title= str(item[i]['title'])\n",
      "            temp_id= str(item[i]['id'])\n",
      "            \n",
      "        except:\n",
      "            pass\n",
      "        example.append(temp)\n",
      "        title.append(temp_id)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Scikit tfidf"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import pipeline\n",
      "# Import all of the scikit learn stuff\n",
      "from __future__ import print_function\n",
      "from sklearn.decomposition import TruncatedSVD\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfTransformer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.preprocessing import Normalizer\n",
      "from sklearn import metrics\n",
      "\n",
      "import pandas as pd\n",
      "import warnings\n",
      "# Suppress warnings from pandas library\n",
      "warnings.filterwarnings(\"ignore\", category=DeprecationWarning,\n",
      "                        module=\"pandas\", lineno=570)\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#example = [\"Machine learning is super fun\",\n",
      "#           \"Python is super, super cool\",\n",
      "#           \"Statistics is cool, too\",\n",
      "#           \"Data science is fun\",\n",
      "#           \"Python is great for machine learning\",\n",
      "#           \"I like football\",\n",
      "#           \"Football is great to watch\"]\n",
      "#vectorizer = TfidfVectorizer(min_df=1, stop_words = 'english')\n",
      "vectorizer = CountVectorizer(min_df = 1, stop_words = 'english')\n",
      "dtm = vectorizer.fit_transform(example)\n",
      "pd.DataFrame(dtm.toarray(),index=title,columns=vectorizer.get_feature_names()).head(5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/lib64/python2.7/site-packages/numpy/core/fromnumeric.py:2499: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
        "  VisibleDeprecationWarning)\n"
       ]
      },
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>00</th>\n",
        "      <th>000</th>\n",
        "      <th>0001pt</th>\n",
        "      <th>00297</th>\n",
        "      <th>004em</th>\n",
        "      <th>04</th>\n",
        "      <th>05</th>\n",
        "      <th>0px</th>\n",
        "      <th>10</th>\n",
        "      <th>100</th>\n",
        "      <th>...</th>\n",
        "      <th>younger</th>\n",
        "      <th>youtube</th>\n",
        "      <th>yu</th>\n",
        "      <th>yuanyang</th>\n",
        "      <th>yun</th>\n",
        "      <th>zealand</th>\n",
        "      <th>zero</th>\n",
        "      <th>zhengzhou</th>\n",
        "      <th>zone</th>\n",
        "      <th>zones</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>172178</th>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td>...</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>172178</th>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td>...</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>172178</th>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td>...</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>172178</th>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td>...</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>172471</th>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td>...</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>5 rows \u00d7 4656 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "        00  000  0001pt  00297  004em  04  05  0px  10  100  ...    younger  \\\n",
        "172178   0    0       0      0      0   0   0    0   0    0  ...          0   \n",
        "172178   0    0       0      0      0   0   0    0   0    0  ...          0   \n",
        "172178   0    0       0      0      0   0   0    0   0    0  ...          0   \n",
        "172178   0    0       0      0      0   0   0    0   0    0  ...          0   \n",
        "172471   0    0       0      0      0   0   0    0   0    0  ...          0   \n",
        "\n",
        "        youtube  yu  yuanyang  yun  zealand  zero  zhengzhou  zone  zones  \n",
        "172178        0   0         0    0        0     0          0     0      0  \n",
        "172178        0   0         0    0        0     0          0     0      0  \n",
        "172178        0   0         0    0        0     0          0     0      0  \n",
        "172178        0   0         0    0        0     0          0     0      0  \n",
        "172471        0   0         0    0        0     0          0     0      0  \n",
        "\n",
        "[5 rows x 4656 columns]"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Each row represents a document. Each column represents a word. So each document is a 13-dim vector.\n",
      "* Each entry equals the number of times the word appears in the document"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get words that correspond to each column\n",
      "#vectorizer.get_feature_names()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "source": [
      "* Example: \"machine\" appears once in the first document, \"super\" appears twice in the second document, and \"statistics\" appears zero times in the third document."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Singular value decomposition and LSA"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim import matutils\n",
      "from gensim import corpora, models, similarities\n",
      "corpus_vect_gensim = matutils.Sparse2Corpus(dtm, documents_columns=False)\n",
      "#lda = models.LdaModel(corpus_vect_gensim, num_topics=60, iterations = 1000)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Fit LSA. Use algorithm = \u201crandomized\u201d for large datasets\n",
      "\n",
      "lsa = TruncatedSVD(2, algorithm = 'arpack')\n",
      "dtm_lsa = lsa.fit_transform(dtm)\n",
      "dtm_lsa = Normalizer(copy=False).fit_transform(dtm_lsa)\n",
      "explained_variance = lsa.explained_variance_ratio_.sum()\n",
      "\n",
      "print(\"Explained variance of the SVD step: {}%\".format(\n",
      "        int(explained_variance * 100)))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Explained variance of the SVD step: 95%\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Each LSA component is a linear combination of words "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#pd.DataFrame(lsa.components_,index = [range(20)],columns = vectorizer.get_feature_names())\n",
      "#pd.DataFrame(dtm_lsa, index = title, columns = range(50))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Each document is a linear combination of the LSA components"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "CLustering using like Affinity propagation, DB Scan "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from sklearn.cluster import DBSCAN, AffinityPropagation\n",
      "import math\n",
      "from sklearn import metrics\n",
      "from sklearn.datasets.samples_generator import make_blobs\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "count =0\n",
      "\n",
      "##############################################################################\n",
      "# Generate sample data\n",
      "centers = [[1, 1], [-1, -1], [1, -1]]\n",
      "#X, labels_true = make_blobs(n_samples=216, centers=centers, cluster_std=0.1,\n",
      "#                            random_state=0)\n",
      "X = dtm_lsa\n",
      "#X=lda\n",
      "true_k = 150\n",
      "\n",
      "##############################################################################\n",
      "# Compute DBSCAN\n",
      "\n",
      "af = DBSCAN(eps=0.95, min_samples=10).fit(X)\n",
      "#core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
      "#core_samples_mask[db.core_sample_indices_] = True\n",
      "\n",
      "\n",
      "#af = AffinityPropagation(damping=.7, max_iter=200, convergence_iter=100, copy=True, preference=None, \n",
      "#                         affinity='euclidean', verbose=False).fit(X)   # Not working for small sized clusters\n",
      "\n",
      "#cluster_centers_indices = af.cluster_centers_indices_\n",
      "#labels = af.labels_\n",
      "#n_clusters_ = len(cluster_centers_indices)\n",
      "\n",
      "\n",
      "#af = MiniBatchKMeans(n_clusters=true_k, init='k-means++', n_init=1,\n",
      "#v                        init_size=1000, batch_size=1000, verbose=False).fit(X)\n",
      "#cluster_centers_indices = km.cluster_centers_indices\n",
      "#labels = af.labels_\n",
      "#n_clusters_ = len(cluster_centers_indices)\n",
      "\n",
      "#km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1,\n",
      "#                verbose=False).fit(X)\n",
      "\n",
      "\n",
      "# Number of clusters in labels, ignoring noise if present.\n",
      "#n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
      "#print('Estimated number of clusters: %d' % n_clusters_)\n",
      "\n",
      "#################################################################################\n",
      "#Print the clusters\n",
      "for k in np.unique(af.labels_):\n",
      "  if(not math.isnan(k)):\n",
      "    members = np.where(af.labels_ == k)  #[1]\n",
      "    if k == -1:\n",
      "        #print(\"outliers:\")\n",
      "        continue    \n",
      "    else:\n",
      "        print(\"cluster %d:\" % int(k))\n",
      "    cluster = []\n",
      "    for item in members[0]:\n",
      "        #print(title[item])\n",
      "        cluster.append(int(title[item]))\n",
      "    print(set(cluster))\n",
      "    #print(X[members])\n",
      "    #print(members)\n",
      "    #pd.DataFrame(X[members], index = example, columns= [range(50)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "cluster 0:\n",
        "set([172160, 171521, 172802, 172675, 171524, 172421, 171529, 172183, 172428, 172558, 172175, 171536, 172178, 171926, 171927, 171844, 172699, 172182, 172152, 172199, 172200, 171509, 171820, 171821, 171823, 172470, 172471, 172472, 171508, 172218, 171707, 172223, 171512, 172738, 171716, 171845, 171846, 171848, 172172, 172215, 171896, 171859, 172185, 171482, 171483, 172764, 171869, 171532, 172256, 171874, 171878, 171881, 171882, 171505, 172658, 172660, 172661, 171894, 172663, 172664, 172668, 171517, 171518])\n",
        "cluster 1:\n",
        "set([171585, 172404, 171590])\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, labels))\n",
      "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\n",
      "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\n",
      "print(\"Adjusted Rand Index: %0.3f\"\n",
      "      % metrics.adjusted_rand_score(labels_true, labels))\n",
      "print(\"Adjusted Mutual Information: %0.3f\"\n",
      "      % metrics.adjusted_mutual_info_score(labels_true, labels))\n",
      "print(\"Silhouette Coefficient: %0.3f\"\n",
      "      % metrics.silhouette_score(X, labels))\n",
      "\"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "##############################################################################\n",
      "# Plot result\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Black removed and is used for noise instead.\n",
      "unique_labels = set(labels)\n",
      "colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n",
      "for k, col in zip(unique_labels, colors):\n",
      "    if k == -1:\n",
      "        # Black used for noise.\n",
      "        col = 'k'\n",
      "\n",
      "    class_member_mask = (labels == k)\n",
      "\n",
      "    xy = X[class_member_mask & core_samples_mask]\n",
      "    plt.plot(y[:, 0], xy[:, 1], 'o', markerfacecolor=col,\n",
      "             markeredgecolor='k', markersize=14)\n",
      "\n",
      "    xy = X[class_member_mask & ~core_samples_mask]\n",
      "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=col,\n",
      "             markeredgecolor='k', markersize=6)\n",
      "\n",
      "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Plot scatter plot of points with vectors\n",
      "#pylab inline\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure()\n",
      "ax = plt.gca()\n",
      "ax.quiver(0,0,xs,ys,angles='xy',scale_units='xy',scale=1, linewidth = .01)\n",
      "ax.set_xlim([-1,1])\n",
      "ax.set_ylim([-1,1])\n",
      "xlabel('First principal component')\n",
      "ylabel('Second principal component')\n",
      "title('Plot of points against LSA principal components')\n",
      "plt.draw()\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Vectorize with TFIDF (term-frequency inverse document-frequency: uses overall frequency of words to weight document-term matrix)\n",
      "* Use LSA components as features in machine learning algorithm: clustering, classification, regression\n",
      "* Alternative dimensionality reduction: Isomap, Random Matrix Methods, Laplacian Eigenmaps, Kernel PCA (cool names!)"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}