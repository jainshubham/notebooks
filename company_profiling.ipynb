{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import print_function\n",
      "import os, sys\n",
      "from time import time\n",
      "os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"web.settings\")\n",
      "sys.path.append('/vagrant/')\n",
      "sys.path.append('/vagrant/contify-banking')\n",
      "\n",
      "from brief.models import Brief  \n",
      "from entry.models import Entry\n",
      "import settings\n",
      "from django.core.management.base import BaseCommand\n",
      "\n",
      "from sklearn.datasets import fetch_20newsgroups\n",
      "from sklearn.decomposition import TruncatedSVD\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, HashingVectorizer\n",
      "from sklearn.pipeline import make_pipeline\n",
      "from sklearn.preprocessing import Normalizer\n",
      "from sklearn import metrics\n",
      "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
      "import logging\n",
      "from optparse import OptionParser\n",
      "from time import time\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from random import random\n",
      "import nltk, math\n",
      "from optparse import make_option           \n",
      "from datetime import datetime, timedelta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.svm import LinearSVC\n",
      "from sklearn.feature_extraction.text import TfidfTransformer\n",
      "from sklearn.multiclass import OneVsRestClassifier\n",
      "from sklearn import preprocessing\n",
      "\n",
      "X_train = np.array([\"new york is a hell of a town\",\n",
      "                    \"new york was originally dutch\",\n",
      "                    \"the big apple is great\",\n",
      "                    \"new york is also called the big apple\",\n",
      "                    \"nyc is nice\",\n",
      "                    \"people abbreviate new york city as nyc\",\n",
      "                    \"the capital of great britain is london\",\n",
      "                    \"london is in the uk\",\n",
      "                    \"london is in england\",\n",
      "                    \"london is in great britain\",\n",
      "                    \"it rains a lot in london\",\n",
      "                    \"london hosts the british museum\",\n",
      "                    \"new york is great and so is london\",\n",
      "                    \"i like london better than new york\"])\n",
      "y_train_text = [[\"new york\"],[\"new york\"],[\"new york\"],[\"new york\"],[\"new york\"],\n",
      "                [\"new york\"],[\"london\"],[\"london\"],[\"london\"],[\"london\"],\n",
      "                [\"london\"],[\"london\"],[\"new york\",\"london\"],[\"new york\",\"london\"]]\n",
      "\n",
      "X_test = np.array(['nice day in nyc',\n",
      "                   'welcome to london',\n",
      "                   'london is rainy',\n",
      "                   'it is raining in britian',\n",
      "                   'it is raining in britian and the big apple',\n",
      "                   'it is raining in britian and nyc',\n",
      "                   'hello welcome to new york. enjoy it here and london too'])\n",
      "target_names = ['New York', 'London']\n",
      "\n",
      "lb = preprocessing.LabelBinarizer()\n",
      "Y = lb.fit_transform(y_train_text)\n",
      "\n",
      "classifier = Pipeline([\n",
      "    ('vectorizer', CountVectorizer()),\n",
      "    ('tfidf', TfidfTransformer()),\n",
      "    ('clf', OneVsRestClassifier(LinearSVC()))])\n",
      "\n",
      "classifier.fit(X_train, Y)\n",
      "predicted = classifier.predict(X_test)\n",
      "all_labels = lb.inverse_transform(predicted)\n",
      "\n",
      "for item, labels in zip(X_test, all_labels):\n",
      "    print '%s => %s' % (item, ', '.join(labels))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#important"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# calculate the most frequent features first\n",
      "vect = TfidfVectorizer(vocabulary=emoticons)\n",
      "matrix = vect.fit_transform(traindata, max_features=10)\n",
      "top_features = vect.vocabulary_\n",
      "n = len(top_features)\n",
      "\n",
      "# insert the emoticons into the vocabulary of common features\n",
      "emoticons = {\":)\":0, \":P\":1, \":(\":2)}\n",
      "for feature, index in emoticons.items():\n",
      "    top_features[feature] = n + index\n",
      "\n",
      "# re-vectorize using both sets of features\n",
      "# at this point len(top_features) == 13\n",
      "vect = TfidfVectorizer(vocabulary=top_features)\n",
      "matrix = vect.fit_transform(traindata)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "SyntaxError",
       "evalue": "invalid syntax (<ipython-input-3-9b71bdcb44f8>, line 8)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-9b71bdcb44f8>\"\u001b[1;36m, line \u001b[1;32m8\u001b[0m\n\u001b[1;33m    emoticons = {\":)\":0, \":P\":1, \":(\":2)}\u001b[0m\n\u001b[1;37m                                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction import text\n",
      "\n",
      "my_additional_stop_words={'span', 'medium', 'border', 'double', 'style', 'class', 'fe00d6','image', 'resize', 'd38833', '00fe64', \n",
      "                          'td', 'padding', '10px', 'valign', 'tr', 'solid', '000000', '1px', 'tr', 'align', 'center',\n",
      "                          'aaa', 'aa', 'td', 'valign', 'tr', 'width', '255', '262', 'midas', 'em', 'h2', 'h3', 'font' 'h4' 'http',\n",
      "                          'img', 'src', 'newsimg', 'png', 'pics2', 'www', 'padding', '10px', '000000', '1px', 'tr', 'center', \n",
      "                          'valign', '255', '262', 'em', '000', 'com', 'alt', 'url', 'sm', 'li', 'elhajj', 'ul', 'wocc', 'xwv',\n",
      "                          'videoid', 'linkro', 'fp7', 'dxb'}\n",
      "#my_additional_stop_words={}   \n",
      "stop_words = text.ENGLISH_STOP_WORDS.union(my_additional_stop_words)\n",
      "item = Entry.objects.order_by(\"approved_on\")[:4000]\n",
      "#tempSoup= str(item[i].title+item[i].section+item[i].topic+item[i].industry+item[i].body_html+item[i].primary_topic+item[i].primary_industry+item[i].auto_tagged_topic+item[i].auto_tagged_industry)\n",
      "storySoup = item.values_list('body_html', flat=True)\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, HashingVectorizer\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vect = TfidfVectorizer()\n",
      "matrix = vect.fit_transform(storySoup)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}