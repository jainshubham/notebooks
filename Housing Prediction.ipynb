{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      " \n",
      "# coding: utf-8\n",
      " \n",
      "#   \n",
      "# ##Predicting the value of a house based on a given data set by Shubham Jain for My City Way. 28.09.2014 \n",
      "# \n",
      "# Language: Python (Scikit-learn, Pandas, matplotlib)\n",
      "# \n",
      "# Algorithms  (Gradient Boosting Regression)\n",
      "# \n",
      "# Discusion\n",
      "# \n",
      "# 1. The given problem is a multivariate regression problem. \n",
      "# 2. The given data is completely numeric with no missing items, therefore there was no need for much pre-processing. \n",
      "# 3. I looked at popular linear, tree based and support vector machine based regression algorithms. These were compared using R2 scores and means squared errors. Non-Linear SVMs and tree based methods performed the best. \n",
      "# 4. Beased on a quick internet search, I found Gradient Boosting Regression performed the best on such data sets and was computationally viable on my laptop as against non linear SVMs and RandomForests.   \n",
      "# 5. Next I needed to select the parameters of GBR for #best fit and least overfitting.\n",
      "# \n",
      "# #####tuned_clf = ensemble.GradientBoostingRegressor(n_estimators=500, max_depth=2, learning_rate=0.01, loss='huber', min_samples_leaf = 3, random_state=0, verbose=1)\n",
      "# \n",
      "# 6.  Feature analysis shows that average number of rooms per dwelling and % lower status of the population are more important featues than others offecting the price of dwelling in a locality. \n",
      " \n",
      "# In[1]:\n",
      " \n",
      "import numpy as np\n",
      "print 'numpy:', np.__version__\n",
      " \n",
      "import scipy\n",
      "print 'scipy:', scipy.__version__\n",
      " \n",
      "import pandas as pd\n",
      "print 'pandas: ',pd.__version__\n",
      " \n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib\n",
      "print 'matplotlib:', matplotlib.__version__\n",
      " \n",
      "import ggplot\n",
      "print 'ggplot:', ggplot.__version__\n",
      " \n",
      "import sklearn \n",
      "from sklearn import datasets, ensemble, metrics, preprocessing\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn import metrics\n",
      "from sklearn.cross_validation import train_test_split\n",
      "print 'scikit-learn:', sklearn.__version__\n",
      " \n",
      "get_ipython().magic(u'pylab inline')\n",
      " \n",
      " "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# ###Read data\n",
      " \n",
      " \n",
      "df = pd.read_csv('/home/shubham/devel/datascience/mycityway/data.txt', header=None, skipinitialspace=True, sep=r\"\\s+\")\n",
      "df.count()\n",
      "df.describe()\n",
      "print\n",
      " \n",
      " \n",
      "# ###Split data into features and target sets\n",
      " \n",
      " \n",
      "df_data = df.ix[:,0:12].values\n",
      "df_target = df[13].values\n",
      " \n",
      " \n",
      "# ### Normalization\n",
      " \n",
      " \n",
      "scaler = preprocessing.StandardScaler().fit(df_data)\n",
      "newdf = pd.DataFrame(scaler.transform(df_data))\n",
      "newdf\n",
      "print\n",
      " \n",
      " "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# ###Compare popular methods \n",
      " \n",
      " \n",
      "#Ensemble\n",
      "gra = sklearn.ensemble.GradientBoostingRegressor(n_estimators=100, max_depth=3, loss='ls')\n",
      "ada = sklearn.ensemble.AdaBoostRegressor()\n",
      "rfr = sklearn.ensemble.RandomForestRegressor()\n",
      "dtr = sklearn.tree.DecisionTreeRegressor()\n",
      " \n",
      "#Linear\n",
      "reg = sklearn.linear_model.LinearRegression()\n",
      "eln = sklearn.linear_model.ElasticNet()\n",
      "bsr = sklearn.linear_model.BayesianRidge()\n",
      "lsr = sklearn.linear_model.Ridge()\n",
      "svr = sklearn.svm.SVR(kernel='linear', C=0.1, epsilon=0.2)\n",
      " \n",
      "cla=(reg, eln, bsr, lsr, svr, gra, ada, rfr, dtr)\n",
      " \n",
      "for clf in cla:\n",
      "    abse=sqe=r2=0\n",
      " \n",
      "    train_x, test_x, train_y, test_y = train_test_split(df_data,df_target, test_size=0.20)\n",
      "    clf = clf.fit(train_x,train_y)\n",
      "    \n",
      "    predict_y = clf.predict(test_x)\n",
      "    abse = metrics.mean_absolute_error(test_y, predict_y)\n",
      "    sqe = metrics.mean_squared_error(test_y, predict_y)\n",
      "    r2 = metrics.r2_score(test_y, predict_y)\n",
      "    \n",
      " \n",
      "    from sklearn.cross_validation import cross_val_score\n",
      "    print clf.__class__.__name__, \"\\t\\t\\t\\t\", r2\n",
      " "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}