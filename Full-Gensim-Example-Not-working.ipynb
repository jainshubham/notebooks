{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os, sys\n",
      "os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"web.settings\")\n",
      "sys.path.append('/vagrant/')\n",
      "#from brief.models import Brief  \n",
      "from entry.models import Entry\n",
      "import nltk\n",
      "from django.core.management.base import BaseCommand\n",
      "from optparse import make_option           \n",
      "from datetime import datetime, timedelta\n",
      "example  = list()\n",
      "title = list()\n",
      "title2 = list()\n",
      "#item = Brief.objects.values()\n",
      "item = Entry.objects.values()\n",
      "\n",
      "for i in range(1500):\n",
      "    #if (item[i]['language'] == \"en\"):\n",
      "        try:\n",
      "            temp= str(item[i]['body_html'])\n",
      "            temp_title= str(item[i]['title'])\n",
      "            temp_id= str(item[i]['id'])\n",
      "            \n",
      "        except:\n",
      "            pass\n",
      "        example.append(temp)\n",
      "        title.append(temp_id)\n",
      "        #title2.append(temp)\n",
      "        #print(title)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import logging, sys, pprint\n",
      "\n",
      "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
      "\n",
      "### Generating a training/background corpus from your own source of documents\n",
      "from gensim.corpora import TextCorpus, MmCorpus, Dictionary\n",
      "\n",
      "# gensim docs: \"Provide a filename or a file-like object as input and TextCorpus will be initialized with a\n",
      "# dictionary in `self.dictionary`and will support the `iter` corpus method. For other kinds of corpora, you only\n",
      "# need to override `get_texts` and provide your own implementation.\"\n",
      "background_corpus = TextCorpus(input=YOUR_CORPUS)\n",
      "\n",
      "# Important -- save the dictionary generated by the corpus, or future operations will not be able to map results\n",
      "# back to original words.\n",
      "background_corpus.dictionary.save(\n",
      "    \"my_dict.dict\")\n",
      "\n",
      "MmCorpus.serialize(\"background_corpus.mm\",\n",
      "    background_corpus)  #  Uses numpy to persist wiki corpus in Matrix Market format. File will be several GBs."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Generating a large training/background corpus using Wikipedia\n",
      "from gensim.corpora import WikiCorpus, wikicorpus\n",
      "\n",
      "articles = \"enwiki-latest-pages-articles.xml.bz2\"  # available from http://en.wikipedia.org/wiki/Wikipedia:Database_download\n",
      "\n",
      "# This will take many hours! Output is Wikipedia in bucket-of-words (BOW) sparse matrix.\n",
      "wiki_corpus = WikiCorpus(articles)\n",
      "wiki_corpus.dictionary.save(\"wiki_dict.dict\")\n",
      "\n",
      "MmCorpus.serialize(\"wiki_corpus.mm\", wiki_corpus)  #  File will be several GBs.\n",
      "\n",
      "### Working with persisted corpus and dictionary\n",
      "bow_corpus = MmCorpus(\"wiki_corpus.mm\")  # Revive a corpus\n",
      "\n",
      "dictionary = Dictionary.load(\"wiki_dict.dict\")  # Load a dictionary\n",
      "\n",
      "### Transformations among vector spaces\n",
      "from gensim.models import LsiModel, LogEntropyModel\n",
      "\n",
      "logent_transformation = LogEntropyModel(wiki_corpus,\n",
      "    id2word=dictionary)  # Log Entropy weights frequencies of all document features in the corpus\n",
      "\n",
      "tokenize_func = wikicorpus.tokenize  # The tokenizer used to create the Wikipedia corpus\n",
      "document = \"Some text to be transformed.\"\n",
      "# First, tokenize document using the same tokenization as was used on the background corpus, and then convert it to\n",
      "# BOW representation using the dictionary created when generating the background corpus.\n",
      "bow_document = dictionary.doc2bow(tokenize_func(\n",
      "    document))\n",
      "# converts a single document to log entropy representation. document must be in the same vector space as corpus.\n",
      "logent_document = logent_transformation[[\n",
      "    bow_document]]\n",
      "\n",
      "# Transform arbitrary documents by getting them into the same BOW vector space created by your training corpus\n",
      "documents = [\"Some iterable\", \"containing multiple\", \"documents\", \"...\"]\n",
      "bow_documents = (dictionary.doc2bow(\n",
      "    tokenize_func(document)) for document in documents)  # use a generator expression because...\n",
      "logent_documents = logent_transformation[\n",
      "                   bow_documents]  # ...transformation is done during iteration of documents using generators, so this uses constant memory\n",
      "\n",
      "### Chained transformations\n",
      "# This builds a new corpus from iterating over documents of bow_corpus as transformed to log entropy representation.\n",
      "# Will also take many hours if bow_corpus is the Wikipedia corpus created above.\n",
      "logent_corpus = MmCorpus(corpus=logent_transformation[bow_corpus])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IOError",
       "evalue": "[Errno 2] No such file or directory: 'enwiki-latest-pages-articles.xml.bz2'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-13-3aa52926219e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# This will take many hours! Output is Wikipedia in bucket-of-words (BOW) sparse matrix.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mwiki_corpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWikiCorpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mwiki_corpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"wiki_dict.dict\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/lib64/python2.7/site-packages/gensim/corpora/wikicorpus.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fname, processes, lemmatize, dictionary, filter_namespaces)\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 270\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_texts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    271\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/lib64/python2.7/site-packages/gensim/corpora/dictionary.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, documents, prune_at)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_documents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprune_at\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprune_at\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/lib64/python2.7/site-packages/gensim/corpora/dictionary.pyc\u001b[0m in \u001b[0;36madd_documents\u001b[1;34m(self, documents, prune_at)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[0mDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m \u001b[0munique\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \"\"\"\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mdocno\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocument\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m             \u001b[1;31m# log progress & run a regular check for pruning, once every 10k docs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdocno\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m10000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/lib64/python2.7/site-packages/gensim/corpora/wikicorpus.pyc\u001b[0m in \u001b[0;36mget_texts\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[0marticles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marticles_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m         \u001b[0mpositions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpositions_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 290\u001b[1;33m         \u001b[0mtexts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpageid\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpageid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mextract_pages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbz2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBZ2File\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter_namespaces\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    291\u001b[0m         \u001b[0mpool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m         \u001b[1;31m# process the corpus in smaller chunks of docs, because multiprocessing.Pool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'enwiki-latest-pages-articles.xml.bz2'"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Creates LSI transformation model from log entropy corpus representation. Takes several hours with Wikipedia corpus.\n",
      "lsi_transformation = LsiModel(corpus=logent_corpus, id2word=dictionary,\n",
      "    num_features=400)\n",
      "\n",
      "# Alternative way of performing same operation as above, but with implicit chaining\n",
      "# lsi_transformation = LsiModel(corpus=logent_transformation[bow_corpus], id2word=dictionary,\n",
      "#    num_features=400)\n",
      "\n",
      "# Can persist transformation models, too.\n",
      "logent_transformation.save(\"logent.model\")\n",
      "lsi_transformation.save(\"lsi.model\")\n",
      "\n",
      "### Similarities (the best part)\n",
      "from gensim.similarities import Similarity\n",
      "\n",
      "# This index corpus consists of what you want to compare future queries against\n",
      "index_documents = [\"A bear walked in the dark forest.\",\n",
      "             \"Tall trees have many more leaves than short bushes.\",\n",
      "             \"A starship may someday travel across vast reaches of space to other stars.\",\n",
      "             \"Difference is the concept of how two or more entities are not the same.\"]\n",
      "# A corpus can be anything, as long as iterating over it produces a representation of the corpus documents as vectors.\n",
      "corpus = (dictionary.doc2bow(tokenize_func(document)) for document in index_documents)\n",
      "\n",
      "index = Similarity(corpus=lsi_transformation[logent_transformation[corpus]], num_features=400, output_prefix=\"shard\")\n",
      "\n",
      "print \"Index corpus:\"\n",
      "pprint.pprint(documents)\n",
      "\n",
      "print \"Similarities of index corpus documents to one another:\"\n",
      "pprint.pprint([s for s in index])\n",
      "\n",
      "query = \"In the face of ambiguity, refuse the temptation to guess.\"\n",
      "sims_to_query = index[lsi_transformation[logent_transformation[dictionary.doc2bow(tokenize_func(query))]]]\n",
      "print \"Similarities of index corpus documents to '%s'\" % query\n",
      "pprint.pprint(sims_to_query)\n",
      "\n",
      "best_score = max(sims_to_query)\n",
      "index = sims_to_query.tolist().index(best_score)\n",
      "most_similar_doc = documents[index]\n",
      "print \"The document most similar to the query is '%s' with a score of %.2f.\" % (most_similar_doc, best_score)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}