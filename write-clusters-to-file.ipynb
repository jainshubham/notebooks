{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import print_function\n",
      "import os, sys\n",
      "from time import time\n",
      "os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"web.settings\")\n",
      "sys.path.append('/vagrant/')\n",
      "sys.path.append('/vagrant/contify-banking')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from brief.models import Brief  \n",
      "from entry.models import Entry\n",
      "import settings\n",
      "from django.core.management.base import BaseCommand\n",
      "\n",
      "from sklearn.datasets import fetch_20newsgroups\n",
      "from sklearn.decomposition import TruncatedSVD\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, HashingVectorizer\n",
      "from sklearn.pipeline import make_pipeline\n",
      "from sklearn.preprocessing import Normalizer\n",
      "from sklearn import metrics\n",
      "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
      "import logging\n",
      "from optparse import OptionParser\n",
      "from sklearn.feature_extraction import text\n",
      "from time import time\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from random import random\n",
      "import nltk, math\n",
      "from optparse import make_option           \n",
      "from datetime import datetime, timedelta\n",
      "start_date = datetime.now()\n",
      "end_date_comparision = start_date - timedelta(hours=48000)\n",
      "t0= time()\n",
      "item = Entry.objects.order_by(\"approved_on\").filter(created_on__gte=end_date_comparision, created_on__lte=start_date)[:1000]\n",
      "storyCols = item.values_list('title', 'title', 'title', 'body_html')\n",
      "storySoup = [' '.join(cols) for cols in storyCols]\n",
      "storyTitle = item.values_list('title')\n",
      "\n",
      "#Vectorizer\n",
      "my_additional_stop_words= {'span', 'medium', 'border', 'double', 'style',\n",
      "                        'class', 'fe00d6','image', 'resize', 'd38833', '00fe64',\n",
      "                        'td', '10px', 'tr', 'solid', '000000', 'align',\n",
      "                        'aaa', 'aa', 'td', 'width', '255', '262', 'midas', \n",
      "                           'img', 'src', 'newsimg', 'png', 'pics2', 'www', \n",
      "                        'valign', '255', '262', 'em', '000', 'com', 'alt', \n",
      "                        'videoid', 'linkro', 'fp7', 'dxb', 'ul',\n",
      "                        'em', 'h2', 'h3', 'font' 'h4' 'http',  \n",
      "                        'wocc', 'a', 'co', 'de', 'fonth4http', 're',\n",
      "                        'noone', 'padding', '1px', 'url', 'sm', 'li', 'elhajj',\n",
      "                        'xwv'}\n",
      "stop_words = text.ENGLISH_STOP_WORDS.union(my_additional_stop_words)\n",
      "hasher = HashingVectorizer(stop_words=stop_words, non_negative=True,\n",
      "                           norm=None, binary=False)\n",
      "vectorizer = make_pipeline(hasher, TfidfTransformer())\n",
      "X = vectorizer.fit_transform(storySoup)\n",
      "print(\"n_samples: %d, n_features: %d\" % X.shape)\n",
      "print('\\x1b[1;31m'+\"Stories Vectorized in %fs\" % (time() - t0)+'\\x1b[0m')\n",
      "\n",
      "#Dimension Reduction\n",
      "lsa = TruncatedSVD(algorithm='arpack', n_components=250)\n",
      "dtm_lsa = lsa.fit_transform(X)\n",
      "X = Normalizer(copy=False).fit_transform(dtm_lsa)\n",
      "print(\"n_samples: %d, n_features: %d\" % X.shape)\n",
      "print('\\x1b[1;31m'+\"Dimensions Reduced in %fs\" % (time() - t0)+'\\x1b[0m')\n",
      "\n",
      "\n",
      "# Do the actual clustering\n",
      "nc = int(len(storySoup)*0.95)\n",
      "km = MiniBatchKMeans(n_clusters= nc, max_iter=100, n_init=5, compute_labels=True, \n",
      "                                init_size=int(nc*3), batch_size=int(nc*.02), \n",
      "                                reassignment_ratio=.1, verbose=False).fit(X)\n",
      "d = km.transform(X)\n",
      "#km = KMeans(n_clusters=nc, init='k-means++', max_iter=100, n_init=10, verbose=True).fit(X)\n",
      "print('\\x1b[1;31m'+\"Stories Clustered in  in %fs\" % (time() - t0)+'\\x1b[0m')\n",
      "\n",
      "#creating a dict iterator form the cluster\n",
      "f = open('testoutput.txt', 'wt')\n",
      "for s in range(len(km.labels_)):\n",
      "    f.write(\"%s,,, %s\\n\" %(storyTitle[s], km.labels_[s]))\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}